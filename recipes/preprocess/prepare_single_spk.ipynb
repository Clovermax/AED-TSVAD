{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac92d3a",
   "metadata": {},
   "source": [
    "# Prepare each speaker's r-vector in each session based on Pyannote for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d430789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Inference\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Timeline\n",
    "from pyannote.database.util import load_rttm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "\n",
    "YOUR_PATH = 'path/to/your/metadata_dir'  # for example, /{BASE_PATH}/AED-TSVAD/recipes/diar_wavlm/data/Compound/dev\n",
    "init_type = 'oracle'  # use oracle rttm for initialization in training\n",
    "\n",
    "def setup_file_logger(log_path: str | Path, level=logging.INFO):\n",
    "    log_path = Path(log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(level)\n",
    "\n",
    "    for h in list(root.handlers):\n",
    "        root.removeHandler(h)\n",
    "\n",
    "    fh = logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\")\n",
    "    fmt = \"%(asctime)s %(levelname)s %(message)s\"\n",
    "    fh.setFormatter(logging.Formatter(fmt))\n",
    "    root.addHandler(fh)\n",
    "\n",
    "setup_file_logger(f\"{YOUR_PATH}/prepare_single_speaker.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110f1ec",
   "metadata": {},
   "source": [
    "## Merge single speaker parts of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareSingleSpeaker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav_scp_path,\n",
    "        rttm_dir,\n",
    "        out_dir,\n",
    "        min_segment_dur: float = 0.3,\n",
    "        min_silence_dur: float = 0.02,\n",
    "        max_silence_dur: float = 0.08,\n",
    "        shuffle_segments: bool = True,\n",
    "    ):\n",
    "        self.audio = Audio(sample_rate=16000, mono=\"downmix\")\n",
    "        self.file_list = self.make_file_list(wav_scp_path, rttm_dir)\n",
    "        self.out_dir = Path(out_dir)\n",
    "\n",
    "        self.min_segment_dur = min_segment_dur\n",
    "        self.min_silence_dur = min_silence_dur\n",
    "        self.max_silence_dur = max_silence_dur\n",
    "        self.shuffle_segments = shuffle_segments\n",
    "\n",
    "    def make_file_list(self, wav_scp_path, rttm_dir):\n",
    "        wav_scp_path = Path(wav_scp_path)\n",
    "        rttm_dir = Path(rttm_dir)\n",
    "        file_list = []\n",
    "\n",
    "        with wav_scp_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                rec_id, wav_path = line.strip().split(maxsplit=1)\n",
    "                item = {\n",
    "                    \"wav_path\": wav_path,\n",
    "                    \"rttm_path\": str(rttm_dir / f\"{rec_id}.rttm\"),\n",
    "                }\n",
    "                file_list.append(item)\n",
    "\n",
    "        return file_list\n",
    "\n",
    "    def extract_clean_single_speaker_timeline(self, annotation, spk):\n",
    "        spk_tl = annotation.label_timeline(spk)\n",
    "\n",
    "        others_tl = Timeline()\n",
    "        for other_spk in annotation.labels():\n",
    "            if other_spk != spk:\n",
    "                others_tl = others_tl | annotation.label_timeline(other_spk)\n",
    "\n",
    "        overlap_with_others = spk_tl.crop(others_tl, mode=\"intersection\")\n",
    "        clean_tl = spk_tl.extrude(overlap_with_others)\n",
    "\n",
    "        return clean_tl\n",
    "\n",
    "    def process_single_file(self, wav_path, rttm_path, out_dir):\n",
    "        rec_id = wav_path.stem\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        ann_by_file = load_rttm(rttm_path)\n",
    "        if rec_id not in ann_by_file:\n",
    "            raise ValueError\n",
    "\n",
    "        ann = ann_by_file[rec_id]\n",
    "        all_tl: Timeline = ann.get_timeline().support()\n",
    "\n",
    "        for spk in ann.labels():\n",
    "            clean_tl = self.extract_clean_single_speaker_timeline(ann, spk)\n",
    "\n",
    "            filtered_segments = [segment for segment in clean_tl if segment.duration > self.min_segment_dur]\n",
    "            if not filtered_segments:\n",
    "                logging.warning(f\"No segments longer than {self.min_segment_dur}s for {rec_id} {spk}\")\n",
    "                continue\n",
    "\n",
    "            if self.shuffle_segments:\n",
    "                random.shuffle(filtered_segments)\n",
    "\n",
    "            wav_chunks = list()\n",
    "            for i, segment in enumerate(filtered_segments):\n",
    "                audio_duration = self.audio.get_duration(wav_path)\n",
    "                if segment.start >= audio_duration:\n",
    "                    logging.warning(f\"Segment [{segment.start:.2f}s, {segment.end:.2f}s] starts after audio file ends ({audio_duration:.2f}s) for {wav_path}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                if segment.end > audio_duration:\n",
    "                    original_end = segment.end\n",
    "                    segment = Segment(start=segment.start, end=audio_duration)\n",
    "                    logging.info(f\"Segment end time clipped from {original_end:.2f}s to {audio_duration:.2f}s for {wav_path}\")\n",
    "\n",
    "                if segment.end - segment.start < self.min_segment_dur:\n",
    "                    logging.warning(f\"Segment [{segment.start:.2f}s, {segment.end:.2f}s] too short after clipping for {wav_path}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                chunk, _ = self.audio.crop(wav_path, segment, mode=\"raise\")\n",
    "                wav_chunks.append(chunk.squeeze(0))\n",
    "\n",
    "                if i < len(filtered_segments) - 1:\n",
    "                    silence_duration = random.uniform(self.min_silence_dur, self.max_silence_dur)\n",
    "                    gap = np.zeros(int(silence_duration * self.audio.sample_rate), dtype=np.float32)\n",
    "                    wav_chunks.append(gap)\n",
    "\n",
    "            spk_wav = np.concatenate(wav_chunks)\n",
    "            out_file = out_dir / f\"{rec_id}-{spk}.wav\"\n",
    "            sf.write(out_file, spk_wav, self.audio.sample_rate)\n",
    "            logging.info(f\"Saved {out_file} ({len(spk_wav)/self.audio.sample_rate:.1f}s)\")\n",
    "\n",
    "    def process(self):\n",
    "        for item in tqdm(self.file_list, ncols=100):\n",
    "            wav_path = Path(item[\"wav_path\"])\n",
    "            rttm_path = Path(item[\"rttm_path\"])\n",
    "            if not wav_path.exists() or not rttm_path.exists():\n",
    "                logging.warning(f\"Missing file: {wav_path} or {rttm_path}\")\n",
    "                continue\n",
    "            self.process_single_file(wav_path, rttm_path, Path(self.out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aede59",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_scp_path = f\"{YOUR_PATH}/wav.scp\"\n",
    "rttm_path = f\"{YOUR_PATH}/init_rttms/{init_type}\"\n",
    "out_dir = f\"{YOUR_PATH}/wavs_single_spk/{init_type}\"\n",
    "\n",
    "preparer = PrepareSingleSpeaker(wav_scp_path, rttm_path, out_dir, min_segment_dur=0.3, shuffle_segments=True)\n",
    "preparer.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86862",
   "metadata": {},
   "source": [
    "## Extract embeddings using Pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSingleSpeakerEmbedding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav_dir,\n",
    "        out_dir,\n",
    "        model_path,\n",
    "        duration=5.0,\n",
    "        step=2.5,\n",
    "        batch_size=32,\n",
    "        device=torch.device(\"cpu\"),\n",
    "    ):\n",
    "        self.wav_dir = Path(wav_dir)\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.duration = duration\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.infer = Inference(\n",
    "            model=model_path,\n",
    "            window=\"sliding\",\n",
    "            duration=duration,\n",
    "            step=step,\n",
    "            skip_aggregation=True,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        # in case chunks are very short\n",
    "        self.model = PretrainedSpeakerEmbedding(embedding=model_path, device=device)\n",
    "\n",
    "        self.file_list = self._make_file_list()\n",
    "\n",
    "    def _make_file_list(self):\n",
    "        file_list = []\n",
    "        for wav_file in sorted(self.wav_dir.glob(\"*.wav\")):\n",
    "            basename = wav_file.stem\n",
    "            file_list.append({\n",
    "                \"wav_path\": wav_file,\n",
    "                \"base_name\": basename\n",
    "            })\n",
    "        return file_list\n",
    "\n",
    "    def crop_to_integer_chunks(self, waveform, sample_rate, duration, step):\n",
    "        total_len = waveform.shape[1]\n",
    "        chunk_size = int(duration * sample_rate)\n",
    "        step_size = int(step * sample_rate)\n",
    "\n",
    "        if total_len < chunk_size:\n",
    "            raise ValueError\n",
    "\n",
    "        num_chunks = 1 + (total_len - chunk_size) // step_size\n",
    "        final_len = (num_chunks - 1) * step_size + chunk_size\n",
    "\n",
    "        residual = total_len - final_len\n",
    "        if residual > step_size // 2:\n",
    "            return np.concatenate([\n",
    "                waveform[:, :final_len],\n",
    "                waveform[:, -chunk_size:]\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            return waveform[:, :final_len]        \n",
    "\n",
    "    def extract_all(self):\n",
    "        for item in tqdm(self.file_list, ncols=100, desc=\"Extracting embeddings\"):\n",
    "            self.extract_one(item[\"wav_path\"], item[\"base_name\"])\n",
    "\n",
    "    def extract_one(self, wav_path, base_name):\n",
    "        out_file = self.out_dir / (base_name + \".pt\")\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        waveform, sr = sf.read(wav_path, dtype='float32')\n",
    "\n",
    "        if waveform.ndim == 1:\n",
    "            waveform = waveform[np.newaxis, :]\n",
    "\n",
    "        if waveform.shape[-1] < self.duration * sr:\n",
    "            logging.info(f\"Waveform length {waveform.shape[-1]} is shorter than chunk size\")\n",
    "            embeddings = self.model(torch.from_numpy(waveform).unsqueeze(dim=0))  # [batch, channels, num_samples]\n",
    "        else:\n",
    "            waveform = self.crop_to_integer_chunks(waveform, sr, self.duration, self.step)\n",
    "            embeddings = self.infer({\"waveform\": torch.from_numpy(waveform), \"sample_rate\": sr})\n",
    "        torch.save(embeddings, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ExtractSingleSpeakerEmbedding(\n",
    "    wav_dir=f\"{YOUR_PATH}/wavs_single_spk/{init_type}\",\n",
    "    out_dir=f\"{YOUR_PATH}/embs_single_spk/{init_type}\",\n",
    "    model_path=\"path/to/your/downloads/pyannote/wespeaker-voxceleb-resnet34-LM/pytorch_model.bin\",\n",
    "    duration=5.0,\n",
    "    step=2.5,\n",
    "    batch_size=32,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "extractor.extract_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notsofar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
